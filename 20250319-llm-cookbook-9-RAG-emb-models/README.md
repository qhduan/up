---
marp: true
paginate: true
theme: gaia
class:
  - lead
  - invert
author: qhduan（AI匠）
---

# LLM Cookbook 第九期  
## RAG系统（模型篇）
## Embedding模型选择与优化
### 作者：AI匠  

---

# 本期内容

- **Embedding模型基础**
- **寻找适合的Embedding模型**
- **模型选择策略与考量**
- **Sparse模型与BM25算法**
- **Embedding模型的未来发展**
- **模型量化与优化**

<!-- 
我是AI匠，欢迎来到LLM Cookbook第九期！
今天我们将深入探讨RAG系统中的核心组件之一：Embedding模型。
我们将从基础概念出发，讨论如何寻找和选择适合的模型，介绍BM25等经典算法，
并展望embedding技术的未来发展方向和优化策略。
这些知识将帮助你构建更高效、更精准的RAG系统！
-->

---

# Embedding模型基础

- **什么是Embedding？**
  - 将文本转换为高维向量表示
  - 捕捉语义信息，而非简单字面匹配
  - 相似文本在向量空间中距离更近


<!-- 
Embedding是RAG系统的基础，它将文本转换为向量表示，使计算机能够"理解"文本的语义。
与传统关键词匹配不同，embedding能够捕捉上下文和语义关系，使得相似含义的文本在向量空间中彼此接近。
这种表示方法使我们能够进行语义搜索，找到与查询在意义上相关的文档，而不仅仅是包含相同关键词的文档。
-->

---

# Embedding模型的应用场景

- **RAG系统中的核心组件**
  - 文档索引：将知识库文档转换为向量
  - 查询理解：将用户问题转换为向量
  - 相似度计算：找到最相关的文档片段

- **其他应用**
  - 文本聚类与分类
  - 推荐系统
  - 异常检测

<!-- 
在RAG系统中，embedding模型扮演着双重角色：一方面将知识库文档转换为向量存储，另一方面将用户查询也转换为向量，然后通过计算向量间的相似度找到最相关的文档。
除了RAG系统，embedding技术还广泛应用于文本聚类、分类、推荐系统等多个领域，是现代NLP应用的基础设施。
-->

---

<style scoped>
  section {
    font-size: 28px;
  }
</style>

# 寻找适合的Embedding模型

## 主要资源平台

- **Hugging Face**
  - 最大的开源模型社区
  - 提供多语言、多领域的embedding模型
  - 便捷的模型下载与使用接口

- **MTEB (Massive Text Embedding Benchmark)**
  - 全面的embedding模型评测基准
  - 涵盖多种任务和语言
  - 帮助选择性能最佳的模型

<!-- 
寻找合适的embedding模型，Hugging Face是首选平台。它提供了丰富的开源模型，从轻量级到大型模型应有尽有。
MTEB是评估embedding模型性能的重要基准，它包含了多种任务和语言的测试，可以帮助我们客观地比较不同模型的性能。
通过这些平台，我们可以根据具体需求选择最适合的模型。
-->

---

# Hugging Face模型示例

<style scoped>
section {
  font-size: 28px;
}
</style>

- **通用多语言模型**
  - `BAAI/bge-m3`
  - `intfloat/multilingual-e5-large`
  - `Alibaba-NLP/gte-Qwen2-7B-instruct`（超大型）

- **中文优化模型**
  - `moka-ai/m3e-base`
  - `BAAI/bge-large-zh-v1.5`

<!-- 
在Hugging Face上，我们可以找到各种各样的embedding模型。对于通用多语言场景，all-MiniLM系列和E5系列是不错的选择；对于中文场景，m3e和bge系列模型表现优异。
使用这些模型非常简单，通过sentence_transformers库，只需几行代码就能生成文本的embedding。
这些预训练模型大大降低了应用门槛，使得构建高质量的RAG系统变得更加容易。
-->

---

# MTEB基准评测

<style scoped>
section {
  font-size: 28px;
}
</style>

- **全面评估模型在不同任务上的表现**
  - 检索任务（Retrieval）
  - 分类任务（Classification）
  - 聚类任务（Clustering）
  - 语义相似度（STS）
  - 可重排序性（Reranking）

- **多语言评测**
  - 英语、中文、德语等多种语言
  - 跨语言能力评估

<!-- 
MTEB是评估embedding模型的重要基准，它通过多种任务和语言来全面测试模型性能。
在选择模型时，可以参考MTEB排行榜，找到在特定任务或语言上表现最好的模型。
比如，如果你的RAG系统主要处理中文文档，那么可以选择在中文检索任务上表现最好的模型。
这种基于客观评测的选择方法，能够帮助我们避免主观判断带来的偏差。
-->

---

# 模型选择策略与考量

<style scoped>
section {
  font-size: 28px;
}
</style>

- **语言需求**
  - 单语言 vs. 多语言模型
  - 领域特定语言（医学、法律等）

- **模型大小与性能权衡**
  - 小型模型：速度快，资源需求低
  - 大型模型：精度高，语义理解更深入，需要大量GPU部署

- **领域适配性**
  - 通用领域 vs. 特定领域模型
  - 考虑是否需要领域适应（Domain Adaptation）

<!-- 
选择embedding模型时，需要考虑多方面因素。首先是语言需求，如果只处理单一语言，专用模型通常效果更好；如果需要处理多语言，则应选择多语言模型。
其次是模型大小与性能的权衡，小型模型速度快但精度可能较低，大型模型精度高但资源消耗大。
最后是领域适配性，通用模型适合广泛场景，但在特定领域（如医学、法律）可能需要专门训练或微调的模型。
根据这些因素综合考量，选择最适合自己应用场景的模型。
-->

---

<style scoped>
section {
  font-size: 28px;
}
</style>

# 单语言 vs. 多语言模型

| 模型类型 | 优势 | 劣势 | 适用场景 |
|---------|------|------|----------|
| **单语言模型** | • 特定语言性能更好<br>• 通常体积更小 | • 不支持跨语言应用<br>• 需要为每种语言选择模型 | • 单一语言应用<br>• 资源受限环境 |
| **多语言模型** | • 支持多种语言<br>• 可进行跨语言检索<br>• 维护成本低 | • 单一语言性能可能略差<br>• 模型通常更大 | • 多语言应用<br>• 国际化产品<br>• 跨语言检索需求 |

<!-- 
单语言模型和多语言模型各有优缺点。单语言模型在特定语言上性能通常更好，且体积更小，适合资源有限的环境；而多语言模型支持多种语言，可以实现跨语言检索，维护成本更低。
选择哪种类型取决于应用需求。如果你的应用只需支持一种语言，且对性能要求高，单语言模型是更好的选择；如果需要支持多种语言，或有跨语言检索需求，则多语言模型更为适合。
-->

---

# Matryoshka Representation Learning (MRL)

<style scoped>
section {
  font-size: 24px;
}
</style>

- **什么是Matryoshka表示学习？**
  - 受俄罗斯套娃启发的嵌套向量表示方法
  - 一个向量包含多个不同维度的子表示

- **核心优势**
  - **灵活维度选择**：根据资源需求动态选择向量维度
  - **向后兼容**：低维度是高维度的子集，无需重新训练
  - **资源效率**：可在低资源环境中使用截断向量

- **应用场景**
  - 资源受限设备上的embedding计算
  - 大规模向量数据库的存储优化
  - 自适应计算复杂度的检索系统

https://arxiv.org/pdf/2205.13147

<!-- 
Matryoshka Representation Learning是一种创新的embedding技术，它受到俄罗斯套娃的启发，创建嵌套的向量表示。
在MRL中，一个完整向量（如1024维）包含多个子向量（如512维、256维等），这些子向量是完整向量的前缀。
这种设计带来三大优势：首先，可以根据资源需求灵活选择向量维度；其次，低维向量是高维向量的子集，保持向后兼容；最后，在资源受限环境中可以使用截断向量，提高效率。
MRL特别适合需要在不同计算能力设备上部署的应用，以及大规模向量数据库的优化。它代表了embedding技术向更灵活、更高效方向发展的趋势。
-->

---

<style scoped>
section {
  font-size: 28px;
}
</style>

# Sparse模型与BM25算法

- **Dense vs. Sparse表示**
  - Dense：稠密向量，每个维度都有值，一般都在256～4096维
  - Sparse：稀疏向量，大多数维度为0，一般在几万到几十万维

- **BM25算法**
  - 经典的信息检索算法
  - 基于TF-IDF的改进版本
  - 考虑词频、文档长度等因素
  - 不需要神经网络，计算高效

- **Hybrid检索（混合检索）**
  - 基于分词的BM25，Dense模型，Sparse模型，这是当前三种主要的召回方法
  - 根据需要结合使用

<!-- 
除了主流的dense embedding模型，sparse模型也是RAG系统中的重要选择。Dense模型捕捉语义关系，而sparse模型更擅长精确匹配关键词。
BM25是一种经典的sparse检索算法，它基于词频和文档长度计算相关性，不需要神经网络，计算效率高，在许多场景下仍然表现出色。
现代RAG系统常采用hybrid检索方法，结合dense和sparse的优势，既能捕捉语义相似性，又能保证关键词匹配的精确度，从而提高整体检索质量。
-->

---

<style scoped>
section {
  font-size: 28px;
}
</style>

# BM25算法原理

- **核心公式**

$$BM25(D,Q) = \sum_{i=1}^{n} IDF(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{avgdl})}$$

- **优势**
  - 考虑词频饱和度（避免过度重视高频词）
  - 文档长度归一化（公平对待长短文档）
  - 计算效率高，易于实现

使用Elasticsearch等全文检索数据库本质上一般等同于使用BM25算法，所以向量数据库可以和全文检索数据库结合使用

<!-- 
BM25算法的核心思想是结合词频、逆文档频率和文档长度来计算文档与查询的相关性。
它的优势在于考虑了词频饱和度，避免过度重视高频词；同时通过文档长度归一化，公平对待长短不一的文档。
截止到今天为止，BM25算法，在很多搜索、相似度场景和一些RAG场景都是最好的算法
-->

---

<style scoped>
section {
  font-size: 28px;
}
</style>

# Embedding模型的未来发展

- **多模态Embedding**
  - 融合文本、图像、音频等多种模态
  - 实现跨模态检索和理解

- **更轻量高效的模型**
  - 降低计算资源需求
  - 提高推理速度

- **超级模型**
  - 基于LLM的超大高质量Embedding模型

- **自适应Embedding**
  - 根据查询动态调整表示方式
  - 提高特定场景下的精确度

<!-- 
Embedding技术正在快速发展，未来趋势主要有三个方向：
首先是多模态embedding，将文本、图像、音频等不同类型的数据映射到同一向量空间，实现跨模态检索。
其次是更轻量高效的模型，通过模型压缩、知识蒸馏等技术，在保持性能的同时降低资源需求。
最后是自适应embedding，能够根据查询内容动态调整表示方式，提高特定场景下的精确度。
这些发展将使RAG系统更加强大、灵活和高效。
-->

---

<style scoped>
section {
  font-size: 28px;
}
</style>

# 模型量化与优化

- **什么是模型量化？**
  - 将模型参数从高精度（如FP32）转换为低精度（如INT8）
  - 减小模型体积，提高推理速度

- **常见量化技术**
  - 后训练量化（Post-Training Quantization）
  - 量化感知训练（Quantization-Aware Training）
  - 混合精度量化（Mixed-Precision Quantization）

- **性能与精度权衡**
  - 通常量化会导致精度轻微下降
  - 在资源受限环境中尤为有价值

<!-- 
模型量化是优化embedding模型的重要技术，它通过降低参数精度来减小模型体积、提高推理速度。
常见的量化技术包括后训练量化、量化感知训练和混合精度量化。后训练量化最为简单，直接将训练好的模型参数转换为低精度；量化感知训练在训练过程中考虑量化效应，精度损失更小；混合精度量化则对不同层使用不同精度。
量化通常会导致精度轻微下降，但在资源受限的环境中，这种权衡是值得的。对于embedding模型，适当的量化几乎不影响检索性能，却能显著提高效率。
现在一些向量数据库支持自动量化，或者提供不同量化级别的索引，所以这部分不一定是一定需要的。
-->

---

<style scoped>
section {
  font-size: 24px;
}
</style>

# 总结与实践建议

- **选择合适的Embedding模型**
  - 根据语言需求、性能要求和资源限制
  - 参考MTEB等基准评测结果(https://huggingface.co/spaces/mteb/leaderboard)
  - 必要时微调领域模型

- **考虑混合检索策略**
  - 结合Dense模型和BM25等Sparse方法

- **注重模型优化**
  - 通过量化等技术提高效率
  - 在保持性能的同时降低资源消耗

- **关注技术发展**
  - 多模态embedding
  - 更轻量高效的模型架构

<!-- 
总结本期内容，构建高效RAG系统的关键在于选择合适的embedding模型、采用混合检索策略、注重模型优化，并关注技术发展趋势。
在实践中，应根据具体应用场景和资源条件，选择最适合的模型和优化策略。无论是通用应用还是特定领域，都有相应的优质模型可供选择。
随着技术不断发展，embedding模型将变得更加强大、高效，为RAG系统提供更坚实的基础。希望本期内容对你构建高质量RAG系统有所帮助！
-->
